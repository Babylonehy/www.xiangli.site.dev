<!DOCTYPE html><html lang="en">
  <head><!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-122620714-1"></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-122620714-1');
		
	</script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no"><title>Dive into Deep Learning Notes(1) - Xiang Li's Space</title>

<meta name="description" content="">
<link rel="canonical" href="https://www.xiangli.site/2021/11/11/D2L.html"><link rel="alternate" type="application/rss+xml" title="Xiang Li's Space" href="/feed.xml"><!-- start favicons snippet, use https://realfavicongenerator.net/ -->
<!--<link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon-16x16.png"><link rel="manifest" href="/assets/site.webmanifest"><link rel="mask-icon" href="/assets/safari-pinned-tab.svg" color="#fc4d50"><link rel="shortcut icon" href="/assets/favicon.ico">

<meta name="msapplication-TileColor" content="#ffc40d"><meta name="msapplication-config" content="/assets/browserconfig.xml">

<meta name="theme-color" content="#ffffff"> -->
<!-- end favicons snippet -->
<link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon-16x16.png">
<link rel="manifest" href="/assets/site.webmanifest">
<link rel="mask-icon" href="/assets/safari-pinned-tab.svg" color="#5bbad5">
<link rel="shortcut icon" href="/assets/favicon.ico">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/browserconfig.xml">
<meta name="theme-color" content="#ffffff"><link rel="stylesheet" href="/assets/css/main.css"><link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/font-awesome/5.15.1/css/all.css" ><!-- start custom head snippets -->

<!-- end custom head snippets -->
<script>(function() {
  window.isArray = function(val) {
    return Object.prototype.toString.call(val) === '[object Array]';
  };
  window.isString = function(val) {
    return typeof val === 'string';
  };

  window.hasEvent = function(event) {
    return 'on'.concat(event) in window.document;
  };

  window.isOverallScroller = function(node) {
    return node === document.documentElement || node === document.body || node === window;
  };

  window.isFormElement = function(node) {
    var tagName = node.tagName;
    return tagName === 'INPUT' || tagName === 'SELECT' || tagName === 'TEXTAREA';
  };

  window.pageLoad = (function () {
    var loaded = false, cbs = [];
    window.addEventListener('load', function () {
      var i;
      loaded = true;
      if (cbs.length > 0) {
        for (i = 0; i < cbs.length; i++) {
          cbs[i]();
        }
      }
    });
    return {
      then: function(cb) {
        cb && (loaded ? cb() : (cbs.push(cb)));
      }
    };
  })();
})();
(function() {
  window.throttle = function(func, wait) {
    var args, result, thisArg, timeoutId, lastCalled = 0;

    function trailingCall() {
      lastCalled = new Date;
      timeoutId = null;
      result = func.apply(thisArg, args);
    }
    return function() {
      var now = new Date,
        remaining = wait - (now - lastCalled);

      args = arguments;
      thisArg = this;

      if (remaining <= 0) {
        clearTimeout(timeoutId);
        timeoutId = null;
        lastCalled = now;
        result = func.apply(thisArg, args);
      } else if (!timeoutId) {
        timeoutId = setTimeout(trailingCall, remaining);
      }
      return result;
    };
  };
})();
(function() {
  var Set = (function() {
    var add = function(item) {
      var i, data = this._data;
      for (i = 0; i < data.length; i++) {
        if (data[i] === item) {
          return;
        }
      }
      this.size ++;
      data.push(item);
      return data;
    };

    var Set = function(data) {
      this.size = 0;
      this._data = [];
      var i;
      if (data.length > 0) {
        for (i = 0; i < data.length; i++) {
          add.call(this, data[i]);
        }
      }
    };
    Set.prototype.add = add;
    Set.prototype.get = function(index) { return this._data[index]; };
    Set.prototype.has = function(item) {
      var i, data = this._data;
      for (i = 0; i < data.length; i++) {
        if (this.get(i) === item) {
          return true;
        }
      }
      return false;
    };
    Set.prototype.is = function(map) {
      if (map._data.length !== this._data.length) { return false; }
      var i, j, flag, tData = this._data, mData = map._data;
      for (i = 0; i < tData.length; i++) {
        for (flag = false, j = 0; j < mData.length; j++) {
          if (tData[i] === mData[j]) {
            flag = true;
            break;
          }
        }
        if (!flag) { return false; }
      }
      return true;
    };
    Set.prototype.values = function() {
      return this._data;
    };
    return Set;
  })();

  window.Lazyload = (function(doc) {
    var queue = {js: [], css: []}, sources = {js: {}, css: {}}, context = this;
    var createNode = function(name, attrs) {
      var node = doc.createElement(name), attr;
      for (attr in attrs) {
        if (attrs.hasOwnProperty(attr)) {
          node.setAttribute(attr, attrs[attr]);
        }
      }
      return node;
    };
    var end = function(type, url) {
      var s, q, qi, cbs, i, j, cur, val, flag;
      if (type === 'js' || type ==='css') {
        s = sources[type], q = queue[type];
        s[url] = true;
        for (i = 0; i < q.length; i++) {
          cur = q[i];
          if (cur.urls.has(url)) {
            qi = cur, val = qi.urls.values();
            qi && (cbs = qi.callbacks);
            for (flag = true, j = 0; j < val.length; j++) {
              cur = val[j];
              if (!s[cur]) {
                flag = false;
              }
            }
            if (flag && cbs && cbs.length > 0) {
              for (j = 0; j < cbs.length; j++) {
                cbs[j].call(context);
              }
              qi.load = true;
            }
          }
        }
      }
    };
    var load = function(type, urls, callback) {
      var s, q, qi, node, i, cur,
        _urls = typeof urls === 'string' ? new Set([urls]) : new Set(urls), val, url;
      if (type === 'js' || type ==='css') {
        s = sources[type], q = queue[type];
        for (i = 0; i < q.length; i++) {
          cur = q[i];
          if (_urls.is(cur.urls)) {
            qi = cur;
            break;
          }
        }
        val = _urls.values();
        if (qi) {
          callback && (qi.load || qi.callbacks.push(callback));
          callback && (qi.load && callback());
        } else {
          q.push({
            urls: _urls,
            callbacks: callback ? [callback] : [],
            load: false
          });
          for (i = 0; i < val.length; i++) {
            node = null, url = val[i];
            if (s[url] === undefined) {
              (type === 'js' ) && (node = createNode('script', { src: url }));
              (type === 'css') && (node = createNode('link', { rel: 'stylesheet', href: url }));
              if (node) {
                node.onload = (function(type, url) {
                  return function() {
                    end(type, url);
                  };
                })(type, url);
                (doc.head || doc.body).appendChild(node);
                s[url] = false;
              }
            }
          }
        }
      }
    };
    return {
      js: function(url, callback) {
        load('js', url, callback);
      },
      css: function(url, callback) {
        load('css', url, callback);
      }
    };
  })(this.document);
})();
</script><script>
  (function() {
    var TEXT_VARIABLES = {
      version: '2.2.6',
      sources: {
        font_awesome: 'https://cdn.bootcdn.net/ajax/libs/font-awesome/5.15.1/css/all.css',
        jquery: 'https://cdn.bootcss.com/jquery/3.1.1/jquery.min.js',
        leancloud_js_sdk: '//cdn.jsdelivr.net/npm/leancloud-storage@3.13.2/dist/av-min.js',
        chart: 'https://cdn.bootcss.com/Chart.js/2.7.2/Chart.bundle.min.js',
        gitalk: {
          js: 'https://unpkg.com/gitalk@latest/dist/gitalk.min.js',
          css: 'https://unpkg.com/gitalk@latest/dist/gitalk.css'
        },
        valine: 'https://unpkg.com/valine/dist/Valine.min.js',
        mathjax: 'https://cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML',
        mermaid: 'https://cdn.bootcss.com/mermaid/8.0.0-rc.8/mermaid.min.js'
      },
      site: {
        toc: {
          selectors: 'h1,h2,h3,h4'
        }
      },
      paths: {
        search_js: '/assets/search.js'
      }
    };
    window.TEXT_VARIABLES = TEXT_VARIABLES;
  })();
</script>
</head>
  <body>
    <div class="root" data-is-touch="false">
      <div class="layout--page js-page-root"><div class="page__main js-page-main page__viewport has-aside cell cell--auto">

      <div class="page__main-inner"><div class="page__header d-print-none"><header class="header"><div class="main">
      <div class="header__title">
        <div class="header__brand"><?xml version="1.0" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 20010904//EN"
 "http://www.w3.org/TR/2001/REC-SVG-20010904/DTD/svg10.dtd">
<svg version="1.0" xmlns="http://www.w3.org/2000/svg"
 width="400.000000pt" height="400.000000pt" viewBox="0 0 400.000000 400.000000"
 preserveAspectRatio="xMidYMid meet">

<g transform="translate(0.000000,400.000000) scale(0.100000,-0.100000)"
fill="#000000" stroke="none">
<path d="M598 3968 c-50 -47 -79 -111 -89 -196 -5 -47 -19 -96 -40 -140 -161
-342 -244 -597 -280 -852 -66 -475 24 -885 266 -1215 25 -33 49 -69 54 -80 5
-11 51 -63 103 -115 99 -99 106 -105 203 -182 33 -26 73 -57 89 -70 16 -13 61
-42 100 -65 39 -24 78 -47 86 -53 8 -5 35 -20 60 -33 178 -94 240 -133 240
-151 0 -7 -32 -35 -71 -62 -40 -27 -95 -66 -123 -86 -28 -20 -64 -44 -81 -54
-16 -9 -32 -20 -35 -23 -8 -11 -299 -201 -318 -208 -9 -4 -19 -10 -22 -14 -9
-13 -127 -69 -146 -69 -10 0 -35 -12 -56 -26 -33 -23 -51 -27 -134 -32 -88 -4
-97 -7 -106 -28 -9 -20 -6 -30 17 -63 15 -22 36 -43 46 -46 11 -4 19 -14 19
-24 0 -65 75 -80 405 -80 272 -1 347 9 432 52 44 23 46 22 40 -18 -6 -36 7
-49 17 -17 9 30 46 245 46 269 0 49 -19 19 -30 -48 l-12 -72 -26 46 c-30 56
-48 54 -20 -3 11 -22 22 -56 25 -77 l5 -38 -103 -34 c-91 -30 -123 -35 -270
-44 -91 -5 -195 -6 -231 -3 l-64 7 -13 92 c-6 51 -15 107 -18 124 -5 30 -3 33
21 33 25 0 132 52 186 90 15 11 36 23 46 27 11 5 66 40 123 78 143 97 173 117
212 142 19 12 41 28 49 35 8 8 47 34 85 58 39 24 81 54 96 67 14 13 30 23 35
23 10 0 94 -122 94 -135 0 -3 7 -17 17 -31 54 -83 82 -252 83 -498 l0 -129 49
6 c93 11 121 50 104 145 -13 70 -7 92 22 92 33 0 52 -23 60 -70 4 -26 12 -40
22 -40 12 0 14 7 8 36 -14 77 -38 104 -94 104 -48 0 -60 -34 -47 -131 8 -63
-8 -93 -55 -104 -29 -7 -30 -5 -45 113 -6 48 -5 52 14 52 24 0 32 -18 32 -77
0 -31 4 -43 13 -40 14 5 11 95 -5 115 -6 6 -21 12 -34 12 -21 0 -24 5 -24 35
0 31 3 35 25 35 14 0 25 6 25 13 0 24 55 67 86 67 34 0 103 -30 122 -53 8 -9
17 -42 21 -73 5 -36 14 -60 24 -66 26 -14 22 -67 -6 -77 -29 -11 -44 -29 -52
-64 -10 -40 27 -38 42 3 14 38 30 38 38 0 4 -17 10 -30 13 -30 4 0 7 123 7
272 l0 273 34 60 c19 33 41 67 48 75 7 8 32 40 55 70 23 30 56 71 73 91 16 19
44 52 60 73 17 20 41 50 55 65 14 15 50 56 80 92 72 84 239 266 279 303 17 17
31 35 31 41 0 6 35 16 81 23 140 20 220 82 259 200 46 135 5 258 -105 319 -14
8 -12 19 13 104 16 51 35 137 42 189 17 129 0 216 -71 358 -60 119 -89 160
-179 252 -186 190 -457 299 -920 369 -394 60 -589 62 -699 6 -26 -12 -57 -26
-71 -30 -14 -4 -41 -18 -60 -30 -19 -12 -66 -41 -105 -64 -147 -89 -251 -186
-393 -367 -82 -106 -101 -138 -109 -182 -12 -69 -8 -361 6 -552 12 -152 16
-170 46 -235 60 -127 85 -184 85 -193 0 -4 20 -39 45 -78 34 -51 38 -60 15
-34 -36 40 -99 130 -119 168 -7 15 -21 36 -31 47 -39 45 -143 277 -159 355 -5
25 -19 88 -30 140 -75 345 -3 823 185 1238 13 29 24 56 24 61 0 5 16 43 36 85
l36 76 44 -31 c24 -16 57 -34 72 -39 47 -16 290 -40 406 -40 68 0 130 -6 157
-14 37 -11 198 -48 275 -62 20 -3 21 -7 12 -55 -5 -28 -7 -57 -5 -63 7 -17
128 -36 229 -36 107 0 121 10 137 90 12 58 13 60 44 60 18 0 70 7 117 15 203
37 194 37 288 14 156 -38 459 -13 544 45 18 12 42 26 53 30 25 11 113 8 188
-5 61 -11 233 -64 251 -78 6 -4 33 -16 60 -26 27 -10 68 -31 92 -46 44 -30
204 -199 204 -216 0 -6 9 -18 21 -29 12 -10 27 -31 35 -46 26 -50 52 -210 65
-396 14 -194 10 -235 -30 -312 -10 -19 -24 -53 -30 -75 -7 -22 -21 -59 -31
-81 -11 -23 -28 -66 -39 -95 -63 -172 -164 -384 -225 -471 -54 -78 -110 -133
-221 -218 -44 -34 -82 -65 -85 -69 -3 -3 -16 -14 -30 -23 -14 -10 -42 -30 -62
-45 -21 -15 -65 -48 -99 -73 -55 -41 -79 -60 -158 -133 -22 -20 -21 -17 7 33
67 121 83 152 80 156 -4 3 -26 -29 -113 -166 -197 -307 -234 -348 -411 -456
-49 -29 -62 -44 -42 -44 9 0 19 4 23 9 3 5 29 21 58 35 28 14 71 37 95 51 l43
24 43 -22 c24 -12 69 -38 100 -57 31 -19 72 -44 91 -55 36 -22 364 -239 380
-253 48 -39 125 -91 125 -83 0 5 -4 13 -10 16 -5 3 -10 10 -10 15 0 15 160
-40 188 -65 2 -2 -7 -23 -19 -47 -29 -55 -41 -208 -17 -208 12 0 17 18 21 78
3 42 12 94 18 116 13 37 14 38 53 33 21 -3 76 1 120 9 45 8 91 14 104 14 28 0
29 -25 2 -42 -26 -17 -25 -22 5 -36 29 -13 32 -34 9 -57 -13 -12 -14 -19 -4
-30 16 -19 1 -39 -49 -65 l-36 -19 29 -1 c36 0 88 55 91 96 1 16 5 37 9 46 5
11 0 24 -13 37 -17 17 -19 24 -9 40 6 10 8 26 5 35 -6 15 -21 16 -157 10 -109
-5 -150 -4 -150 4 0 21 -50 44 -164 77 -76 21 -126 41 -148 59 -29 25 -81 62
-173 126 -16 11 -67 46 -112 78 -46 31 -106 71 -133 87 -40 23 -115 70 -148
91 -1 1 17 30 40 64 24 35 53 80 65 101 36 65 185 199 220 199 31 0 187 -144
300 -275 28 -33 54 -57 57 -54 3 3 13 43 21 90 16 93 19 244 6 302 -5 20 -5
39 -1 42 9 6 133 -52 173 -80 15 -11 53 -39 83 -62 31 -24 61 -43 67 -43 19 0
-1 143 -36 255 -19 61 -30 111 -26 113 20 8 197 -90 259 -144 14 -11 34 -24
44 -28 60 -21 -56 464 -162 677 -21 44 -31 75 -26 85 7 18 25 62 67 173 14 37
35 86 46 110 10 24 25 62 31 83 7 22 22 55 33 74 18 31 19 45 14 160 -7 141
-51 414 -92 567 -45 169 -85 308 -102 355 -8 25 -19 86 -23 135 -13 154 -16
160 -86 222 -75 66 -117 93 -145 93 -18 0 -19 -2 -9 -15 7 -8 17 -15 22 -15 6
0 26 -12 44 -27 19 -16 50 -41 69 -57 51 -41 77 -98 83 -181 l4 -70 -28 38
c-111 147 -145 184 -229 252 -93 76 -91 75 -239 75 -110 0 -135 -3 -140 -15
-3 -8 -10 -15 -17 -15 -6 0 -9 7 -5 15 6 17 -9 21 -20 4 -3 -6 -1 -16 5 -22
17 -17 13 -31 -4 -17 -8 7 -15 21 -15 31 0 11 -4 19 -10 19 -5 0 -10 -6 -10
-12 0 -10 -2 -10 -9 0 -5 9 -23 12 -52 10 -42 -3 -43 -4 -19 -15 37 -18 94
-73 119 -114 34 -58 38 -119 10 -176 -71 -149 -220 -203 -423 -153 -218 53
-298 140 -275 298 10 64 36 117 66 135 20 12 15 27 -10 27 -16 0 -32 -14 -55
-51 -29 -43 -36 -66 -48 -157 -8 -59 -15 -141 -16 -183 l-2 -76 -90 -17 c-263
-49 -596 -43 -811 13 l-110 29 0 109 c0 59 -6 137 -14 172 -21 98 -87 161
-168 161 l-38 -1 48 -23 c128 -63 172 -176 100 -259 -35 -41 -142 -91 -251
-118 -69 -17 -91 -18 -148 -9 -144 23 -169 50 -159 167 4 43 15 92 23 108 l16
30 7 -38 c9 -54 46 -88 103 -94 27 -3 59 0 77 7 29 13 31 16 26 56 -4 40 -3
43 14 33 11 -5 19 -14 19 -18 0 -20 31 -19 54 2 28 25 57 106 49 136 -5 20
-12 21 -152 21 l-147 0 -36 -32z m2402 -66 c0 -10 5 -23 12 -30 9 -9 9 -12 0
-12 -6 0 -18 14 -27 30 -14 27 -14 30 0 30 8 0 15 -8 15 -18z m134 -18 c3 -8
2 -12 -4 -9 -6 3 -10 10 -10 16 0 14 7 11 14 -7z m26 -50 c0 -8 -4 -12 -10 -9
-5 3 -10 10 -10 16 0 5 5 9 10 9 6 0 10 -7 10 -16z m-33 -46 c-3 -8 -6 -5 -6
6 -1 11 2 17 5 13 3 -3 4 -12 1 -19z m-110 -90 c-3 -7 -5 -2 -5 12 0 14 2 19
5 13 2 -7 2 -19 0 -25z m217 6 c10 -25 -1 -29 -14 -5 -6 12 -7 21 -1 21 5 0
12 -7 15 -16z m-2087 -46 c-3 -7 -5 -2 -5 12 0 14 2 19 5 13 2 -7 2 -19 0 -25z
m1863 5 c0 -9 6 -13 15 -9 8 3 15 1 15 -4 0 -6 -5 -10 -12 -10 -9 0 -9 -3 0
-12 17 -17 15 -29 -3 -22 -8 4 -15 15 -15 25 0 10 -4 19 -9 19 -6 0 -9 9 -8
19 1 23 17 17 17 -6z m-1881 -49 c-1 -30 -5 -51 -8 -46 -3 5 -13 12 -21 15
-12 4 -12 6 2 6 11 1 17 8 16 19 -2 28 2 62 7 62 3 0 5 -25 4 -56z m-45 11
c-19 -14 -19 -14 0 -15 11 0 17 -2 14 -5 -3 -3 -19 -2 -37 1 -32 7 -32 7 -9
19 33 19 57 19 32 0z m1161 -14 c3 -5 1 -12 -5 -16 -5 -3 -10 1 -10 9 0 18 6
21 15 7z m-1240 -21 c3 -5 -3 -10 -14 -10 -10 0 -22 -6 -24 -12 -4 -8 -6 -7
-6 4 -1 23 32 37 44 18z m61 -8 c-10 -2 -22 0 -28 6 -6 6 0 7 19 4 21 -5 23
-7 9 -10z m1228 3 c3 -8 2 -15 -3 -15 -5 0 -14 7 -21 15 -10 12 -10 15 3 15 8
0 17 -7 21 -15z m856 5 c0 -5 -2 -10 -4 -10 -3 0 -8 5 -11 10 -3 6 -1 10 4 10
6 0 11 -4 11 -10z m77 -6 c-3 -3 -12 -4 -19 -1 -8 3 -5 6 6 6 11 1 17 -2 13
-5z m-2164 -21 c-13 -2 -33 -2 -45 0 -13 2 -3 4 22 4 25 0 35 -2 23 -4z m1154
-15 c-3 -8 -6 -5 -6 6 -1 11 2 17 5 13 3 -3 4 -12 1 -19z m107 -3 c-6 -14 -44
-22 -44 -10 0 4 29 19 47 24 1 1 0 -6 -3 -14z m69 -12 c3 -5 -6 -9 -20 -10
-19 -1 -22 1 -12 8 17 11 26 11 32 2z m20 -30 c-13 -2 -33 -2 -45 0 -13 2 -3
4 22 4 25 0 35 -2 23 -4z m-611 -58 c9 -10 -9 -97 -23 -107 -14 -9 -177 -4
-236 8 -27 5 -33 11 -33 33 0 14 3 36 6 49 6 21 10 22 143 22 76 0 140 -2 143
-5z m-279 -373 c-2 -10 -13 -24 -24 -31 -19 -11 -18 -9 2 18 26 36 29 37 22
13z m-43 -87 c0 -5 -5 -3 -10 5 -5 8 -10 20 -10 25 0 6 5 3 10 -5 5 -8 10 -19
10 -25z m-24 -40 c-6 -14 -12 -20 -14 -15 -6 16 -22 12 -22 -5 0 -8 -4 -15
-10 -15 -30 0 10 55 41 57 14 1 15 -2 5 -22z m909 -87 c-3 -54 -3 -57 5 -23
l8 40 -3 -40 c-1 -22 -6 -51 -10 -65 -5 -20 -5 -22 5 -10 10 13 11 12 6 -3 -3
-10 -6 -25 -6 -33 0 -23 -15 -16 -17 9 -1 12 -2 33 -3 47 -5 50 3 140 11 140
5 0 7 -28 4 -62z m-1374 15 c-12 -20 -14 -14 -5 12 4 9 9 14 11 11 3 -2 0 -13
-6 -23z m539 -1 c0 -4 -9 -14 -20 -22 -15 -11 -19 -22 -15 -41 7 -30 -26 -81
-39 -61 -4 7 -1 12 7 12 16 0 29 44 16 57 -5 5 2 20 14 35 22 27 37 35 37 20z
m-110 -23 c0 -5 -4 -9 -10 -9 -5 0 -10 7 -10 16 0 8 5 12 10 9 6 -3 10 -10 10
-16z m-20 -44 c0 -14 -4 -25 -10 -25 -11 0 -14 33 -3 43 11 11 13 8 13 -18z
m70 -14 c0 -6 -4 -13 -10 -16 -5 -3 -10 1 -10 9 0 9 5 16 10 16 6 0 10 -4 10
-9z m590 -6 c0 -8 -4 -15 -10 -15 -5 0 -10 7 -10 15 0 8 5 15 10 15 6 0 10 -7
10 -15z m267 -7 c-3 -8 -6 -5 -6 6 -1 11 2 17 5 13 3 -3 4 -12 1 -19z m-953
-116 c-5 -31 -9 -39 -12 -24 -2 12 -8 20 -13 17 -5 -4 -9 -1 -9 5 0 7 7 9 16
6 15 -6 16 1 11 56 -4 55 -4 58 5 25 6 -22 7 -57 2 -85z m933 81 c-3 -10 -5
-4 -5 12 0 17 2 24 5 18 2 -7 2 -21 0 -30z m43 -12 c0 -6 -4 -13 -10 -16 -5
-3 -10 1 -10 9 0 9 5 16 10 16 6 0 10 -4 10 -9z m-1383 -33 c-3 -8 -6 -5 -6 6
-1 11 2 17 5 13 3 -3 4 -12 1 -19z m1234 -203 c39 -77 59 -102 59 -75 0 5 13
24 28 42 31 35 91 157 93 186 1 9 12 -17 25 -58 15 -48 27 -112 31 -178 7
-120 10 -120 73 -1 23 45 41 68 44 59 8 -24 38 -132 47 -170 18 -69 49 -347
51 -442 1 -54 5 -98 10 -98 4 0 8 4 8 9 0 5 6 16 13 23 7 7 26 36 42 63 46 81
51 85 115 85 75 0 116 -27 153 -99 33 -67 34 -114 3 -203 -28 -79 -56 -108
-145 -150 -51 -23 -84 -31 -158 -37 l-93 -6 -58 -58 c-31 -31 -76 -72 -99 -89
-87 -68 -130 -98 -141 -98 -6 0 -23 -10 -39 -23 -53 -44 -171 -90 -329 -131
-79 -21 -257 -29 -369 -17 -113 12 -331 64 -387 92 -21 10 -43 19 -50 19 -8 0
-45 16 -83 36 -194 100 -316 201 -360 299 -5 11 -9 130 -10 265 -1 238 -2 247
-28 325 -15 44 -27 103 -27 130 -1 52 60 325 72 325 11 0 14 -58 6 -150 -3
-47 -4 -87 -2 -89 8 -8 131 129 150 167 11 20 29 51 40 68 l21 33 11 -98 c17
-148 24 -156 82 -96 19 19 39 35 44 35 12 0 144 126 185 176 13 16 28 39 34
51 6 13 13 23 16 23 3 0 6 -37 6 -83 0 -64 5 -92 23 -129 12 -26 22 -52 22
-57 0 -13 38 -21 107 -21 81 0 153 37 251 130 39 37 122 152 122 170 0 5 3 10
8 10 4 0 15 -35 26 -78 17 -71 59 -162 75 -162 21 0 148 160 165 208 39 106
37 105 57 25 9 -40 37 -111 60 -158z m-864 193 c-3 -8 -6 -5 -6 6 -1 11 2 17
5 13 3 -3 4 -12 1 -19z m1000 -20 c-3 -7 -5 -2 -5 12 0 14 2 19 5 13 2 -7 2
-19 0 -25z m20 10 c-3 -8 -6 -5 -6 6 -1 11 2 17 5 13 3 -3 4 -12 1 -19z
m-1400 -130 c-2 -24 -4 -5 -4 42 0 47 2 66 4 43 2 -24 2 -62 0 -85z m-383 58
c-1 -21 -6 -42 -12 -48 -7 -7 -8 2 -1 32 8 33 7 45 -3 52 -10 7 -10 9 2 5 10
-3 15 -16 14 -41z m1753 22 c-3 -8 -6 -5 -6 6 -1 11 2 17 5 13 3 -3 4 -12 1
-19z m-972 -67 c-5 -19 -8 -22 -15 -11 -6 10 -10 6 -13 -16 -2 -16 -2 -4 0 26
3 30 8 59 11 64 9 16 23 -37 17 -63z m-884 32 c-12 -20 -14 -14 -5 12 4 9 9
14 11 11 3 -2 0 -13 -6 -23z m-26 -25 c-2 -19 1 -36 8 -40 9 -5 7 -11 -7 -21
-17 -13 -18 -21 -8 -138 9 -107 14 -134 43 -194 46 -98 118 -189 143 -179 27
10 44 -164 26 -262 -22 -116 -17 -209 15 -279 26 -58 99 -139 173 -193 43 -30
203 -122 214 -122 5 0 19 -7 31 -15 62 -40 304 -104 462 -122 50 -5 95 -25 95
-43 0 -4 -28 -22 -62 -40 -35 -17 -83 -43 -108 -59 l-45 -28 -45 32 c-25 17
-79 51 -120 75 -41 23 -87 51 -102 61 -15 11 -30 19 -34 19 -10 0 -126 72
-161 100 -17 14 -61 47 -97 75 -36 27 -66 53 -66 57 0 4 -31 44 -70 90 -71 85
-75 90 -133 168 -32 44 -97 176 -97 197 0 13 -32 75 -42 83 -45 34 -82 715
-43 791 19 37 35 29 30 -13z m1885 32 c0 -5 -2 -10 -4 -10 -3 0 -8 5 -11 10
-3 6 -1 10 4 10 6 0 11 -4 11 -10z m-22 -46 c0 -19 -3 -23 -10 -13 -7 10 -8 7
-4 -10 4 -18 2 -21 -6 -13 -9 9 -8 18 2 37 17 31 18 31 18 -1z m-1834 1 c-4
-8 -7 -15 -9 -15 -2 0 -5 7 -9 15 -3 9 0 15 9 15 9 0 12 -6 9 -15z m2300 -77
c-2 -3 0 -9 6 -13 12 -7 11 -77 -1 -120 -5 -20 -6 -6 -3 41 3 40 1 74 -4 75
-5 2 -13 20 -16 39 -7 34 -7 35 8 10 8 -14 12 -28 10 -32z m-1432 -14 c-8 -15
-10 -15 -15 0 -4 9 -2 16 3 16 6 0 11 10 11 23 l2 22 4 -22 c2 -12 0 -30 -5
-39z m-378 20 c3 -8 2 -12 -4 -9 -6 3 -10 10 -10 16 0 14 7 11 14 -7z m-493
-23 c6 10 9 3 9 -21 0 -24 -3 -31 -9 -21 -7 11 -10 10 -14 -5 -2 -10 -5 4 -4
31 0 28 2 39 4 26 4 -19 7 -21 14 -10z m1799 9 c0 -5 -5 -10 -11 -10 -5 0 -7
5 -4 10 3 6 8 10 11 10 2 0 4 -4 4 -10z m-1303 -92 c-2 -13 -4 -5 -4 17 -1 22
1 32 4 23 2 -10 2 -28 0 -40z m1762 5 c-2 -26 -2 -27 -6 -4 -4 18 -7 21 -13
10 -5 -7 -12 -10 -15 -7 -10 10 4 28 21 28 9 0 14 -10 13 -27z m-2240 -50 c1
-12 -4 -25 -9 -28 -7 -4 -8 3 -4 21 5 21 4 25 -6 19 -10 -6 -12 -4 -7 9 4 10
7 23 8 29 1 19 18 -26 18 -50z m2198 15 c-3 -8 -6 -5 -6 6 -1 11 2 17 5 13 3
-3 4 -12 1 -19z m-2187 -78 c0 -5 -2 -10 -4 -10 -3 0 -8 5 -11 10 -3 6 -1 10
4 10 6 0 11 -4 11 -10z m2287 -77 c-3 -10 -5 -2 -5 17 0 19 2 27 5 18 2 -10 2
-26 0 -35z m893 -308 c0 -5 11 -37 24 -70 48 -119 114 -360 131 -479 5 -39 4
-47 -7 -40 -19 11 -92 68 -98 76 -22 28 -191 118 -224 118 -27 0 -26 -17 10
-106 19 -49 39 -107 43 -129 25 -113 31 -149 26 -154 -5 -5 -26 13 -82 72 -14
15 -30 27 -34 27 -4 0 -27 13 -51 28 -24 15 -71 41 -106 57 -82 38 -89 30 -67
-64 14 -55 17 -100 13 -211 -2 -77 -8 -143 -12 -147 -7 -7 -16 2 -59 58 -17
24 -109 129 -128 148 -29 28 -110 96 -120 99 -18 7 -8 19 64 72 39 30 80 61
89 70 9 8 34 28 55 43 75 55 219 171 243 196 54 56 108 142 167 261 34 69 64
134 68 144 5 17 9 15 30 -20 13 -21 25 -43 25 -49z m-1030 -462 c0 -4 -33 -41
-73 -82 -40 -42 -104 -111 -142 -154 -70 -79 -85 -96 -137 -159 -16 -18 -37
-44 -49 -58 -12 -14 -27 -34 -35 -44 -14 -21 -24 -19 -164 28 -25 8 -57 17
-72 20 -44 10 -37 43 10 52 20 3 68 19 106 35 37 16 72 29 76 29 8 0 109 58
155 90 87 59 158 116 225 179 66 63 100 85 100 64z m-729 -470 c1 -32 3 -50 6
-40 2 9 8 17 13 17 13 0 20 -28 20 -84 0 -66 -27 -109 -90 -143 -81 -43 -179
-11 -214 70 -15 34 -16 47 -6 89 13 51 26 62 33 26 3 -18 4 -16 5 7 1 17 -5
44 -12 60 l-13 30 101 11 c171 19 156 23 157 -43z m-1377 -741 c3 -5 8 -50 12
-100 l7 -92 -48 6 c-71 8 -108 30 -101 59 5 18 1 24 -17 29 -31 8 -75 47 -79
71 -3 18 4 20 92 26 52 3 103 6 112 7 9 1 19 -2 22 -6z"/>
<path d="M2205 2204 c-22 -3 -62 -9 -89 -14 -27 -6 -92 -15 -144 -20 -52 -6
-101 -15 -109 -21 -17 -14 -17 -48 0 -55 7 -2 66 5 132 16 66 12 184 24 263
27 91 4 141 10 139 16 -13 40 -92 61 -192 51z"/>
<path d="M778 2177 c-16 -13 -28 -25 -28 -28 0 -4 35 -9 78 -12 42 -4 139 -19
214 -33 141 -27 168 -27 168 1 0 18 -39 35 -78 35 -15 0 -79 13 -142 30 -138
36 -176 37 -212 7z"/>
<path d="M965 2050 c-110 -13 -225 -52 -225 -77 0 -7 6 -13 14 -13 8 0 16 -5
18 -12 3 -8 11 -7 28 5 31 22 50 22 50 0 0 -15 2 -14 15 4 32 45 115 37 115
-12 0 -28 -43 -65 -74 -65 -25 0 -51 18 -59 40 -10 27 -7 -45 3 -106 15 -84
29 -114 67 -151 38 -38 39 -40 16 -47 -10 -3 3 -3 30 0 26 3 76 5 112 5 36 0
65 4 65 10 0 5 -7 9 -15 9 -19 0 -19 9 -1 29 29 34 49 147 43 237 l-6 85 44
-7 c37 -5 45 -3 45 9 0 16 -31 28 -115 45 -82 17 -107 19 -170 12z m150 -106
c0 -13 -8 -20 -27 -22 -32 -4 -39 21 -10 37 23 14 37 8 37 -15z m-75 2 c0 -16
-18 -31 -27 -22 -8 8 5 36 17 36 5 0 10 -6 10 -14z m98 -95 c-2 -29 -8 -37
-25 -39 -22 -3 -33 -22 -13 -22 15 0 12 -45 -5 -85 -14 -36 -46 -55 -91 -55
-18 0 -31 10 -45 35 -12 19 -27 35 -35 35 -8 0 -14 10 -14 25 0 28 9 32 29 13
10 -11 11 -10 7 2 -8 21 11 49 22 33 13 -20 59 -34 70 -20 27 35 26 40 -6 34
-36 -7 -67 17 -41 34 8 5 20 9 27 9 6 0 12 5 12 10 0 16 58 38 86 32 22 -4 25
-9 22 -41z"/>
<path d="M2051 2041 c-89 -23 -191 -70 -191 -88 0 -19 20 -16 55 7 17 11 33
20 37 20 4 0 7 -46 7 -102 0 -115 16 -175 56 -216 14 -14 25 -29 25 -33 0 -5
8 -9 18 -9 15 -1 15 -2 -2 -15 -11 -8 -17 -17 -15 -20 3 -2 41 0 85 6 43 6
105 11 137 11 32 0 55 4 52 9 -4 5 -15 9 -26 9 -25 0 -24 8 5 39 43 47 60 122
53 238 l-7 102 30 -6 c19 -3 30 -1 30 6 0 6 7 11 15 11 8 0 15 4 15 10 0 17
-75 31 -190 36 -90 3 -131 0 -189 -15z m69 -56 c19 -23 3 -63 -33 -85 -39 -24
-65 -25 -95 -4 -31 21 -27 48 11 78 36 29 97 35 117 11z m158 -8 c4 -21 -25
-34 -40 -19 -8 8 -8 16 2 27 16 19 34 15 38 -8z m-76 -2 c0 -5 -5 -11 -11 -13
-6 -2 -11 4 -11 13 0 9 5 15 11 13 6 -2 11 -8 11 -13z m102 -61 c10 -4 16 -18
16 -40 0 -38 -20 -54 -72 -54 -28 -1 -31 -2 -17 -13 25 -19 39 -52 39 -91 0
-31 -6 -40 -42 -66 -43 -32 -84 -38 -121 -18 -26 13 -67 55 -67 68 0 4 -5 10
-12 12 -16 6 -1 58 17 58 8 0 15 4 15 9 0 19 33 20 57 1 34 -26 47 -25 72 6
21 27 21 27 1 59 -18 29 -18 33 -4 53 12 17 25 22 59 22 24 0 50 -3 59 -6z
m-124 -94 c0 -5 -2 -10 -4 -10 -2 0 -11 -3 -19 -6 -8 -3 -17 2 -20 10 -5 12 0
16 18 16 14 0 25 -4 25 -10z"/>
<path d="M2738 1794 c-17 -16 -6 -24 31 -24 56 0 57 -8 15 -108 -10 -26 -30
-50 -51 -63 -35 -22 -43 -39 -17 -39 40 0 102 80 129 165 6 18 9 16 28 -17 26
-45 51 -51 41 -10 -4 15 -22 42 -40 61 -26 27 -42 35 -81 38 -26 3 -51 1 -55
-3z"/>
<path d="M848 1543 c-26 -31 -30 -43 -14 -43 9 0 16 8 16 18 0 10 5 23 12 30
7 7 10 12 7 12 -3 0 -13 -8 -21 -17z"/>
<path d="M976 1544 c-4 -9 -4 -19 -1 -22 2 -3 7 3 11 12 4 9 4 19 1 22 -2 3
-7 -3 -11 -12z"/>
<path d="M2307 1536 c-4 -10 -5 -21 -2 -24 9 -9 17 6 13 25 -3 17 -4 17 -11
-1z"/>
<path d="M2351 1532 c-13 -25 -14 -52 -2 -52 10 0 26 58 18 67 -3 2 -10 -4
-16 -15z"/>
<path d="M2236 1524 c-9 -22 3 -32 14 -14 12 19 12 30 1 30 -5 0 -12 -7 -15
-16z"/>
<path d="M2130 1509 c-13 -24 -4 -41 10 -19 10 16 13 40 5 40 -2 0 -9 -9 -15
-21z"/>
<path d="M1581 1433 c-268 -4 -281 -6 -281 -64 0 -28 57 -89 83 -89 7 1 -5 15
-26 33 -34 30 -49 69 -29 81 4 2 124 6 267 7 l260 3 3 -28 c3 -20 -6 -38 -35
-69 -46 -52 -118 -87 -195 -96 -62 -8 -90 -21 -42 -21 130 0 242 54 283 134
11 24 21 48 21 53 0 17 -41 63 -53 61 -7 -2 -122 -4 -256 -5z"/>
<path d="M1342 2580 c0 -14 2 -19 5 -12 2 6 2 18 0 25 -3 6 -5 1 -5 -13z"/>
<path d="M2357 3968 c-9 -19 -15 -54 -14 -88 2 -47 8 -63 35 -97 27 -35 41
-43 78 -49 66 -10 141 0 165 22 18 16 19 26 14 84 l-6 65 31 -57 c18 -34 37
-58 46 -58 9 0 31 16 49 36 34 37 51 102 39 150 l-6 24 -208 0 -208 0 -15 -32z"/>
<path d="M2380 272 c0 -5 13 -15 29 -22 16 -6 39 -22 51 -35 19 -21 82 -149
97 -197 3 -10 14 -18 25 -18 11 0 18 3 16 8 -2 4 -18 36 -36 72 -18 36 -38 73
-44 82 -18 25 -3 33 25 15 36 -23 283 -112 448 -161 31 -9 80 -16 110 -15 53
1 53 1 19 11 -19 5 -62 18 -95 29 -33 11 -94 31 -135 44 -130 41 -337 120
-415 158 -65 31 -95 41 -95 29z"/>
<path d="M1802 171 c2 -7 7 -12 11 -12 12 1 9 15 -3 20 -7 2 -11 -2 -8 -8z"/>
<path d="M1797 99 c-4 -13 -7 -41 -7 -61 0 -31 4 -38 20 -38 17 0 20 7 20 43
0 24 -6 51 -13 62 -13 17 -14 17 -20 -6z"/>
<path d="M3510 112 c0 -4 16 -14 36 -21 44 -16 42 -41 -6 -59 l-35 -13 38 5
c75 10 88 45 27 76 -42 21 -60 25 -60 12z"/>
</g>
</svg>
<a title="Feed  body with  food, feed your soul with love." href="/">Xiang Li's Space</a></div><button class="button button--secondary button--circle search-button js-search-toggle"><i class="fas fa-search"></i></button></div><nav class="navigation">
        <ul><li class="navigation__item"><a href="/archive.html">Archive</a></li><li class="navigation__item"><a href="/about.html">About</a></li><li><button class="button button--secondary button--circle search-button js-search-toggle"><i class="fas fa-search"></i></button></li></ul>
      </nav></div>
  </header>
</div><div class="page__content"><div class ="main"><div class="grid grid--reverse">

              <div class="col-aside d-print-none js-col-aside"><aside class="page__aside js-page-aside"><div class="toc-aside js-toc-root"></div>
</aside></div>

              <div class="col-main cell cell--auto"><!-- start custom main top snippet -->

<!-- end custom main top snippet -->
<article itemscope itemtype="http://schema.org/Article"><div class="article__header"><header><h1>Dive into Deep Learning Notes(1)</h1></header></div><meta itemprop="headline" content="Dive into Deep Learning Notes(1)"><div class="article__info clearfix"><ul class="left-col menu"><li>
              <a class="button button--secondary button--pill button--sm"
                href="/archive.html?tag=d2l">d2l</a>
            </li><li>
              <a class="button button--secondary button--pill button--sm"
                href="/archive.html?tag=notes">notes</a>
            </li></ul><ul class="right-col menu"><li><i class="far fa-calendar-alt"></i> <span>Nov 11, 2021</span>
            </li><li><i class="far fa-eye"></i> <span class="js-pageview" data-page-key="/2021/11/11/D2L">0</span> views</li></ul></div><meta itemprop="author" content="Xiang Li (Sean)"/><meta itemprop="datePublished" content="2021-11-11T00:00:00+08:00">
    <meta itemprop="keywords" content="d2l,notes"><div class="js-article-content"><div class="layout--article"><!-- start custom article top snippet -->

<!-- end custom article top snippet -->
<div class="article__content" itemprop="articleBody"><!--more-->

<h2 id="3-linear-neural-networks"><a href="https://d2l.ai/chapter_linear-networks/index.html">3. Linear Neural Networks</a></h2>

<h3 id="linear-regression-8">Linear Regression [8]</h3>

<ul>
  <li>
    <p>MiniBatch SGD</p>
  </li>
  <li>
    <p><a href="https://github.com/sovrasov/flops-counter.pytorch">CNN 模型所需的计算力（flops）和参数（parameters)</a></p>
  </li>
  <li>
    <p>matrix-derivatives</p>
    <ul>
      <li><a href="https://wzbtech.com/tech/matrix-derivatives1.html">https://wzbtech.com/tech/matrix-derivatives1.html</a></li>
      <li><a href="https://www.cnblogs.com/pinard/p/10825264.html">https://www.cnblogs.com/pinard/p/10825264.html</a></li>
    </ul>
  </li>
</ul>

<h3 id="softmax">Softmax</h3>

\[\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{其中}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}\]

<p>Numerical stability: \(\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o-max(o)})\quad \text{其中}\quad \hat{y}_j = \frac{\exp(o_j-max(o))}{\sum_k \exp(o_k-max(0))}\)</p>

\[\begin{aligned}
\log{(\hat y_j)} &amp; = \log\left( \frac{\exp(o_j)}{\sum_k \exp(o_k)}\right) \\
&amp; = \log{(\exp(o_j))}-\log{\left( \sum_k \exp(o_k) \right)} \\
&amp; = o_j -\log{\left( \sum_k \exp(o_k) \right)}.
\end{aligned}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">X_exp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">partition</span> <span class="o">=</span> <span class="n">X_exp</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X_exp</span> <span class="o">/</span> <span class="n">partition</span>  <span class="c1"># The broadcasting mechanism is applied here
</span></code></pre></div></div>

<h3 id="log-likelihood">Log-Likelihood</h3>

<p>最大化一个似然函数同最大化它的自然对数是等价的。因为自然对数log是一个连续且在似然函数的值域内严格递增的上凹函数。<strong><em>注意：可能性函数（似然函数）的自然对数跟信息熵以及Fisher信息联系紧密。</em></strong>求对数通常能够一定程度上简化运算<sup id="fnref:MLE" role="doc-noteref"><a href="#fn:MLE" class="footnote" rel="footnote">1</a></sup></p>

<div align="center"> <img class="image image--lg" src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Logarithm.svg/1920px-Logarithm.svg.png" /> </div>

<p>信息论的核心思想是量化数据中的信息内容，在信息论中，该数值被称为分布$P$的<em>熵</em>（entropy）。可以通过以下方程得到：</p>

\[H[P] = \sum_j - P(j) \log P(j).\]

<p>当我们赋予一个事件较低的概率时，我们的惊异会更大。克劳德·香农决定用 $\log \frac{1}{P(j)} = -\log P(j)$ 来量化一个人的<em>惊异</em>（surprisal）。</p>

<p>In short, we can think of the cross-entropy classification objective in two ways:</p>

<ul>
  <li>(i) as maximizing the likelihood of the observed data;</li>
  <li>(ii) as minimizing our surprisal (and thus the number of bits) required to communicate the labels.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_hat</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)),</span> <span class="n">y</span><span class="p">])</span>
</code></pre></div></div>

<h2 id="5-deep-learning-computation"><a href="https://d2l.ai/chapter_deep-learning-computation/model-construction.html">5. Deep Learning Computation</a></h2>

<h3 id="cpu-gpu-tpu-fpga-ai-asic-3132">CPU, GPU, TPU, FPGA, AI ASIC [31，32]</h3>

<p><strong>CPU:</strong> 可以处理通用计算，性能优化考虑数据读写效率和多线程</p>

<p><strong>GPU:</strong> 使用更多的小核和更好的内存带宽，适合能大规模并行处理的计算任务</p>

<p><strong>DSP: 数字信号处理</strong></p>

<ul>
  <li>为数字信号处理算法设计：点积， 卷积， FFT</li>
  <li>低功耗，高性能</li>
  <li>VLIW: Very Long instrction word
    <ul>
      <li>一条指令计算上百次乘累加</li>
    </ul>
  </li>
  <li>编程和调试困难</li>
  <li>编译器质量良莠不齐</li>
</ul>

<p><strong>FPGA: 可编程阵列</strong></p>

<ul>
  <li>有大量可编程逻辑单元和可配置的连接</li>
  <li>可以配置成计算复杂函数
    <ul>
      <li>编程语言 VHDL, Verilog</li>
    </ul>
  </li>
  <li>比通用硬件更高效</li>
  <li>编译太久</li>
</ul>

<p><strong>AI ASIC</strong></p>

<ul>
  <li>Google TPU
    <ul>
      <li>媲美Nvidia GPU</li>
      <li>核心是 systolic array(大矩阵乘法) 专门为矩阵乘法定制硬件</li>
    </ul>
  </li>
  <li>FPGA 可以用于验证asic设计</li>
</ul>

<p>Transformer 适用于TPU，GPU由于内存限制，所以需要单独设计
卖芯片得有生态</p>

<p>Risc-V 主要是低端芯片，便宜，生态不够完善</p>

<h3 id="单机多卡分布式-33-34-35">单机多卡/分布式 [33, 34, 35]</h3>

<ul>
  <li>数据并行
    <ul>
      <li>当数据多样性够多的情况下，大的batchsize会不收敛，因为相似的样本计算梯度意义不大</li>
    </ul>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="k">def</span> <span class="nf">print_fn</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>  <span class="c1"># @save
</span>    <span class="k">def</span> <span class="nf">wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"[{}]:"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">func</span><span class="p">.</span><span class="n">__name__</span><span class="p">),</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kw</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">wrapper</span>

<span class="o">@</span><span class="n">print_fn</span>
<span class="k">def</span> <span class="nf">try_gpu</span><span class="p">(</span><span class="n">i</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>  <span class="c1"># @save
</span>    <span class="s">"""如果存在，则返回gpu(i)，否则返回cpu()。"""</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s">'cuda:</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cpu'</span><span class="p">)</span>

<span class="o">@</span><span class="n">print_fn</span>
<span class="k">def</span> <span class="nf">try_all_gpus</span><span class="p">():</span>  <span class="c1"># @save
</span>    <span class="s">"""返回所有可用的GPU，如果没有GPU，则返回[cpu(),]。"""</span>
    <span class="n">devices</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s">'cuda:</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
               <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">device_count</span><span class="p">())]</span>
    <span class="k">return</span> <span class="n">devices</span> <span class="k">if</span> <span class="n">devices</span> <span class="k">else</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cpu'</span><span class="p">)]</span>

<span class="n">try_gpu</span><span class="p">()</span>
<span class="n">try_gpu</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">try_all_gpus</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">[</span>try_gpu]: cuda:0
<span class="o">[</span>try_gpu]: cpu
<span class="o">[</span>try_all_gpus]: <span class="o">[</span>device<span class="o">(</span><span class="nb">type</span><span class="o">=</span><span class="s1">'cuda'</span>, <span class="nv">index</span><span class="o">=</span>0<span class="o">)</span>, device<span class="o">(</span><span class="nb">type</span><span class="o">=</span><span class="s1">'cuda'</span>, <span class="nv">index</span><span class="o">=</span>1<span class="o">)</span>, device<span class="o">(</span><span class="nb">type</span><span class="o">=</span><span class="s1">'cuda'</span>, <span class="nv">index</span><span class="o">=</span>2<span class="o">)</span>, device<span class="o">(</span><span class="nb">type</span><span class="o">=</span><span class="s1">'cuda'</span>, <span class="nv">index</span><span class="o">=</span>3<span class="o">)</span>, device<span class="o">(</span><span class="nb">type</span><span class="o">=</span><span class="s1">'cuda'</span>, <span class="nv">index</span><span class="o">=</span>4<span class="o">)</span>, device<span class="o">(</span><span class="nb">type</span><span class="o">=</span><span class="s1">'cuda'</span>, <span class="nv">index</span><span class="o">=</span>5<span class="o">)</span>, device<span class="o">(</span><span class="nb">type</span><span class="o">=</span><span class="s1">'cuda'</span>, <span class="nv">index</span><span class="o">=</span>6<span class="o">)</span>, device<span class="o">(</span><span class="nb">type</span><span class="o">=</span><span class="s1">'cuda'</span>, <span class="nv">index</span><span class="o">=</span>7<span class="o">)]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_batch_ch13</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">devices</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="c1"># 微调BERT中所需（稍后讨论）
</span>        <span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">net</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">trainer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">l</span><span class="p">.</span><span class="nb">sum</span><span class="p">().</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">trainer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">train_loss_sum</span> <span class="o">=</span> <span class="n">l</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>
    <span class="n">train_acc_sum</span> <span class="o">=</span> <span class="n">d2l</span><span class="p">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_loss_sum</span><span class="p">,</span> <span class="n">train_acc_sum</span>
    
<span class="k">def</span> <span class="nf">train_ch13</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_iter</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span>
               <span class="n">devices</span><span class="o">=</span><span class="n">d2l</span><span class="p">.</span><span class="n">try_all_gpus</span><span class="p">()):</span>
    <span class="n">timer</span><span class="p">,</span> <span class="n">num_batches</span> <span class="o">=</span> <span class="n">d2l</span><span class="p">.</span><span class="n">Timer</span><span class="p">(),</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_iter</span><span class="p">)</span>
    <span class="n">animator</span> <span class="o">=</span> <span class="n">d2l</span><span class="p">.</span><span class="n">Animator</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s">'epoch'</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">],</span> <span class="n">ylim</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                            <span class="n">legend</span><span class="o">=</span><span class="p">[</span><span class="s">'train loss'</span><span class="p">,</span> <span class="s">'train acc'</span><span class="p">,</span> <span class="s">'test acc'</span><span class="p">])</span>
    <span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="n">devices</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="c1"># 4个维度：储存训练损失，训练准确度，实例数，特点数
</span>        <span class="n">metric</span> <span class="o">=</span> <span class="n">d2l</span><span class="p">.</span><span class="n">Accumulator</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_iter</span><span class="p">):</span>
            <span class="n">timer</span><span class="p">.</span><span class="n">start</span><span class="p">()</span>
            <span class="n">l</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="n">train_batch_ch13</span><span class="p">(</span>
                <span class="n">net</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">devices</span><span class="p">)</span>
            <span class="n">metric</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">acc</span><span class="p">,</span> <span class="n">labels</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">labels</span><span class="p">.</span><span class="n">numel</span><span class="p">())</span>
            <span class="n">timer</span><span class="p">.</span><span class="n">stop</span><span class="p">()</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="p">(</span><span class="n">num_batches</span> <span class="o">//</span> <span class="mi">5</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">==</span> <span class="n">num_batches</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">animator</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_batches</span><span class="p">,</span>
                             <span class="p">(</span><span class="n">metric</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">metric</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="n">metric</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">metric</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
                              <span class="bp">None</span><span class="p">))</span>
        <span class="n">test_acc</span> <span class="o">=</span> <span class="n">d2l</span><span class="p">.</span><span class="n">evaluate_accuracy_gpu</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">test_iter</span><span class="p">)</span>
        <span class="n">animator</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="n">test_acc</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'loss </span><span class="si">{</span><span class="n">metric</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">metric</span><span class="p">[</span><span class="mi">2</span><span class="p">]:.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">, train acc '</span>
          <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">metric</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">metric</span><span class="p">[</span><span class="mi">3</span><span class="p">]:.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">, test acc </span><span class="si">{</span><span class="n">test_acc</span><span class="p">:.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">metric</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_epochs</span> <span class="o">/</span> <span class="n">timer</span><span class="p">.</span><span class="nb">sum</span><span class="p">():.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> examples/sec on '</span>
          <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li>模型并行</li>
</ul>

<h3 id="pytorch">Pytorch</h3>

<h4 id="mysequential">MySequential</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MySequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">args</span><span class="p">:</span>
            <span class="c1"># 这里，`block`是`Module`子类的一个实例。我们把它保存在'Module'类的成员变量
</span>            <span class="c1"># `_modules` 中。`block`的类型是OrderedDict。
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">block</span><span class="p">]</span> <span class="o">=</span> <span class="n">block</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># OrderedDict保证了按照成员添加的顺序遍历它们
</span>        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">_modules</span><span class="p">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">X</span>
</code></pre></div></div>

<h4 id="initialization">Initialization</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">init_normal</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">:</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="n">bias</span><span class="p">)</span>
<span class="n">net</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">init_normal</span><span class="p">)</span>
<span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">bias</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<h4 id="shared-weight">Shared Weight</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 我们需要给共享层一个名称，以便可以引用它的参数。
</span><span class="n">shared</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
                    <span class="n">shared</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
                    <span class="n">shared</span><span class="p">,</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
                    <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="c1"># 检查参数是否相同
</span><span class="k">print</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">net</span><span class="p">[</span><span class="mi">4</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">net</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span>
<span class="c1"># 确保它们实际上是同一个对象，而不只是有相同的值。
</span><span class="k">print</span><span class="p">(</span><span class="n">net</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">net</span><span class="p">[</span><span class="mi">4</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<h4 id="自定义层">自定义层</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CenteredLayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
<span class="k">class</span> <span class="nc">MyLinear</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_units</span><span class="p">,</span> <span class="n">units</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">in_units</span><span class="p">,</span> <span class="n">units</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">units</span><span class="p">,))</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">linear</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">data</span>
        <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">linear</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="fileio">File/I/O</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s">'x-file'</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'x-file'</span><span class="p">)</span>

<span class="n">torch</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s">'mlp.params'</span><span class="p">)</span>
<span class="n">clone</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">()</span>
<span class="n">clone</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'mlp.params'</span><span class="p">))</span>
<span class="n">clone</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="4-multilayer-perceptrons"><a href="https://d2l.ai/chapter_multilayer-perceptrons/mlp.html">4. Multilayer Perceptrons</a></h2>

<p>为了发挥多层结构的潜力，我们还需要一个额外的关键要素：在仿射变换之后对每个隐藏单元应用非线性的<em>激活函数</em>（activation function）$\sigma$。激活函数的输出（例如，$\sigma(\cdot)$）被称为<em>激活值</em>（activations）。一般来说，有了激活函数，就不可能再将我们的多层感知机退化成线性模型：</p>

\[\begin{aligned}

    \mathbf{H} &amp; = \sigma(\mathbf{X} \mathbf{W}^{(1)} + \mathbf{b}^{(1)}), \\
    \mathbf{O} &amp; = \mathbf{H}\mathbf{W}^{(2)} + \mathbf{b}^{(2)}.\\

\end{aligned}\]

<p>训练感知机(Perceptrons)等价于使用batchsize为1的梯度下降，并且使用如下的损失函数:</p>

\[max(0, -y \times &lt;\mathbf{w}, \mathbf{x}&gt;)\]

<p>如果分错了，更新梯度，如果分对了，不更新梯度。 Perceptrons不能拟合XOR函数，它只能产生线性分割面。[1969]导致了AI寒冬。</p>

<h3 id="activation-functions">Activation Functions</h3>

<p><strong><em>Sigmoid</em></strong>： <strong>对于一个定义域在$\mathbb{R}$中的输入，<em>sigmoid函数</em>将输入变换为区间(0, 1)上的输出</strong>。因此，sigmoid通常称为<em>挤压函数</em>（squashing function）：它将范围(-inf, inf)中的任意输入压缩到区间(0, 1)中的某个值：</p>

\[\operatorname{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.\]

<div align="center"><img src="https://d2l.ai/_images/output_mlp_76f463_42_0.svg" width="  " /></div>

<p>sigmoid函数的导数为下面的公式：</p>

\[\frac{d}{dx} \operatorname{sigmoid}(x) = \frac{\exp(-x)}{(1 + \exp(-x))^2} = \operatorname{sigmoid}(x)\left(1-\operatorname{sigmoid}(x)\right).\]

<div align="center"><img src="https://d2l.ai/_images/output_mlp_76f463_54_0.svg" width="  " /></div>

<p><strong><em>Tanh</em></strong>: Like the sigmoid function, the tanh (hyperbolic tangent) function also squashes its inputs, transforming them into elements on the interval between -1 and 1:</p>

\[\operatorname{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)} = \frac{exp(x) - \exp(-x)}{exp(x) + \exp(-x)}.\]

<div align="center"><img src="https://d2l.ai/_images/output_mlp_76f463_66_0.svg" width="  " /></div>

<p>函数商的求导法: \(\frac{f(x)}{g(x)} = \frac{f'(x)g(x)-f(x)g'(x)}{g(x)^2}\)</p>

<p>tanh函数的导数是：</p>

\[\frac{d}{dx} \operatorname{tanh}(x) = 1 - \operatorname{tanh}^2(x).\]

<div align="center"><img src="https://d2l.ai/_images/output_mlp_76f463_78_0.svg" width="  " /></div>

<p><strong><em>Relu(rectified linear unit (ReLU))</em></strong>: [<strong>ReLU提供了一种非常简单的非线性变换</strong>]。
给定元素$x$，ReLU函数被定义为该元素与$0$的最大值：</p>

\[\operatorname{ReLU}(x) = \max(x, 0).\]

<div align="center"><img src="https://d2l.ai/_images/output_mlp_76f463_18_0.svg" width="  " /></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
</code></pre></div></div>

<p>使用ReLU的原因是，它求导表现得特别好：要么让参数消失，要么让参数通过。这使得优化表现得更好，并且ReLU减轻了困扰以往神经网络的梯度消失问题。</p>

<p>注意，ReLU函数有许多变体，包括<em>参数化ReLU</em>（Parameterized ReLU，<em>pReLU</em>）函数 <code class="language-plaintext highlighter-rouge">He. Zhang. Ren.ea.2015</code> 。该变体为ReLU添加了一个线性项，因此即使参数是负的，某些信息仍然可以通过：</p>

\[\operatorname{pReLU}(x) = \max(0, x) + \alpha \min(0, x).\]

<div align="center"><img src="https://production-media.paperswithcode.com/methods/new_prelu.jpg" width="  " /></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Flatten</span><span class="p">(),</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
                    <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">不能将神经网络网络的权重初始化为相同值，如果是 Relu 此时无法通过SGD更新除最后一层bias外的参数进行学习。但是其他激活函数仍然可以更新</code> {:.warning}</p>

<h3 id="model-selection-underfitting-and-overfitting-11">Model Selection, Underfitting, and Overfitting [11]</h3>

<h4 id="weight-decay">Weight Decay</h4>

<p><strong><em>通过限制参数取值范围来控制模型容量</em></strong>
在训练参数化机器学习模型时，<em>权重衰减</em>（通常称为$L_2$正则化）是最广泛使用的正则化的技术之一。
实际上，我们通过<em>正则化常数</em>$\lambda$来描述这种权衡，这是一个非负超参数，我们使用验证数据拟合：</p>

\[L(\mathbf{w}, b) + \frac{\lambda}{2} \|\mathbf{w}\|^2,\]

<p>对于$\lambda = 0$，我们恢复了原来的损失函数。对于$\lambda &gt; 0$，我们限制$| \mathbf{w} |$的大小。我们仍然除以$2$ 当我们取一个二次函数的导数时，$2$和$1/2$会抵消，以确保更新表达式看起来既漂亮又简单。聪明的读者可能会想知道为什么我们使用平方范数而不是标准范数（即欧几里得距离）。我们这样做是为了便于计算。通过平方$L_2$范数，我们去掉平方根，留下权重向量每个分量的平方和。这使得惩罚的导数很容易计算：导数的和等于和的导数。
使用$L_2$范数，而不是$L_1$范数。事实上，这些选择在整个统计领域中都是有效的和受欢迎的。$L_2$正则化线性模型构成经典的<em>岭回归</em>（ridge regression）算法，$L_1$正则化线性回归是统计学中类似的基本模型，通常被称为<em>套索回归</em>（lasso regression）。</p>

<p>使用$L_2$范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。在实践中，这可能使它们对单个变量中的观测误差更为鲁棒。相比之下，$L_1$惩罚会导致模型将其他权重清除为零而将权重集中在一小部分特征上。这称为<em>特征选择</em>（feature selection），这可能是其他场景下需要的。较小的$\lambda$值对应较少约束的$\mathbf{w}$，而较大的$\lambda$值对$\mathbf{w}$的约束更大。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">l2_penalty</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">w</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="mi">2</span>
 <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">lambd</span> <span class="o">*</span> <span class="n">l2_penalty</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

<span class="c1"># The bias parameter has not decayed
</span>    <span class="n">trainer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">([{</span>
        <span class="s">"params"</span><span class="p">:</span> <span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">weight</span><span class="p">,</span>
        <span class="s">'weight_decay'</span><span class="p">:</span> <span class="n">wd</span><span class="p">},</span> <span class="p">{</span>
            <span class="s">"params"</span><span class="p">:</span> <span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">bias</span><span class="p">}],</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="dropout">Dropout</h4>

<p>在标准dropout正则化中，通过按保留（未丢弃）的节点的分数进行归一化来消除每一层的偏差。换言之，每个中间激活值$h$以<em>丢弃概率</em>$p$由随机变量$h’$替换，如下所示：</p>

\[\begin{aligned}
h' =
\begin{cases}

    0 &amp; \text{ 概率为 } p \\
    \frac{h}{1-p} &amp; \text{ 其他情况}

\end{cases}
\end{aligned}\]

<p>根据设计，期望值保持不变，即$E[h’] = h$。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">dropout_layer</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dropout</span><span class="p">):</span>
    <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">dropout</span> <span class="o">&lt;=</span> <span class="mi">1</span>
    <span class="c1"># In this case, all elements are dropped out
</span>    <span class="k">if</span> <span class="n">dropout</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="c1"># In this case, all elements are kept
</span>    <span class="k">if</span> <span class="n">dropout</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">X</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">dropout</span><span class="p">).</span><span class="nb">float</span><span class="p">()</span>
    <span class="c1"># mask = (torch.Tensor(X.shape).uniform_(0, 1) &gt; dropout).float()
</span>    <span class="k">return</span> <span class="n">mask</span> <span class="o">*</span> <span class="n">X</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">dropout</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_outputs</span><span class="p">,</span> <span class="n">num_hiddens1</span><span class="p">,</span> <span class="n">num_hiddens2</span><span class="p">,</span>
                 <span class="n">is_training</span> <span class="o">=</span> <span class="bp">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_inputs</span> <span class="o">=</span> <span class="n">num_inputs</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">training</span> <span class="o">=</span> <span class="n">is_training</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lin1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_hiddens1</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lin2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_hiddens1</span><span class="p">,</span> <span class="n">num_hiddens2</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lin3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">num_hiddens2</span><span class="p">,</span> <span class="n">num_outputs</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">H1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">lin1</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_inputs</span><span class="p">))))</span>
        <span class="c1"># 只有在训练模型时才使用dropout
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">training</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span>
            <span class="c1"># 在第一个全连接层之后添加一个dropout层
</span>            <span class="n">H1</span> <span class="o">=</span> <span class="n">dropout_layer</span><span class="p">(</span><span class="n">H1</span><span class="p">,</span> <span class="n">dropout1</span><span class="p">)</span>
        <span class="n">H2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">lin2</span><span class="p">(</span><span class="n">H1</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">training</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span>
            <span class="c1"># 在第二个全连接层之后添加一个dropout层
</span>            <span class="n">H2</span> <span class="o">=</span> <span class="n">dropout_layer</span><span class="p">(</span><span class="n">H2</span><span class="p">,</span> <span class="n">dropout2</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lin3</span><span class="p">(</span><span class="n">H2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Flatten</span><span class="p">(),</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="c1"># 在第一个全连接层之后添加一个dropout层
</span>        <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout1</span><span class="p">),</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="c1"># 在第二个全连接层之后添加一个dropout层
</span>        <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout2</span><span class="p">),</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</code></pre></div></div>

<h4 id="梯度消失和梯度爆炸">梯度消失和梯度爆炸</h4>

<p>考虑一个具有$L$层、输入$\mathbf{x}$和输出$\mathbf{o}$的深层网络。每一层$l$由变换$f_l$定义，该变换的参数为权重$\mathbf{W}^{(l)}$，其隐藏变量是$\mathbf{h}^{(l)}$（令 $\mathbf{h}^{(0)} = \mathbf{x}$）。我们的网络可以表示为：</p>

\[\mathbf{h}^{(l)} = f_l (\mathbf{h}^{(l-1)}) \text{ 因此 } \mathbf{o} = f_L \circ \ldots \circ f_1(\mathbf{x}).\]

<p>如果所有隐藏变量和输入都是向量，我们可以将$\mathbf{o}$关于任何一组参数$\mathbf{W}^{(l)}$的梯度写为下式：</p>

\[\partial_{\mathbf{W}^{(l)}} \mathbf{o} = \underbrace{\partial_{\mathbf{h}^{(L-1)}} \mathbf{h}^{(L)}}_{ \mathbf{M}^{(L)} \stackrel{\mathrm{def}}{=}} \cdot \ldots \cdot \underbrace{\partial_{\mathbf{h}^{(l)}} \mathbf{h}^{(l+1)}}_{ \mathbf{M}^{(l+1)} \stackrel{\mathrm{def}}{=}} \underbrace{\partial_{\mathbf{W}^{(l)}} \mathbf{h}^{(l)}}_{ \mathbf{v}^{(l)} \stackrel{\mathrm{def}}{=}}.\]

<p>换言之，该梯度是$L-l$个矩阵$\mathbf{M}^{(L)} \cdot \ldots \cdot \mathbf{M}^{(l+1)}$与梯度向量 $\mathbf{v}^{(l)}$的乘积。因此，我们容易受到数值下溢问题的影响，当将太多的概率乘在一起时，这些问题经常会出现。在处理概率时，一个常见的技巧是切换到对数空间，即将数值表示的压力从尾数转移到指数。不幸的是，我们上面的问题更为严重：最初，矩阵 $\mathbf{M}^{(l)}$ 可能具有各种各样的特征值。他们可能很小，也可能很大，他们的乘积可能<em>非常大</em>，也可能<em>非常小</em>。</p>

<p>不稳定梯度带来的风险不止在于数值表示。不稳定梯度也威胁到我们优化算法的稳定性。我们可能面临一些问题。
要么是 <em>梯度爆炸</em>（gradient exploding）问题：参数更新过大(lr太大)，破坏了模型的稳定收敛；
要么是 <em>梯度消失</em>（gradient vanishing）问题：参数更新过小(lr太小)，在每次更新时几乎不会移动，导致无法学习。</p>

<p><strong><em>让训练更加稳定</em></strong>：</p>

<ul>
  <li>Goal： 让梯度在合理的范围之内，例如[1e-6, 1e3]</li>
  <li>将乘法变加法： ResNet，LSTM</li>
  <li>归一化： 梯度归一化，梯度裁剪</li>
  <li>合理的权重初始化和激活函数</li>
</ul>

<h4 id="xavier初始化">Xavier初始化</h4>

<p>让我们看看某些<em>没有非线性</em>的全连接层输出(例如，隐藏变量)$o_{i}$的尺度分布。
对于该层$n_\mathrm{in}$输入$x_j$及其相关权重$w_{ij}$，输出由下式给出</p>

\[o_{i} = \sum_{j=1}^{n_\mathrm{in}} w_{ij} x_j.\]

<p>权重$w_{ij}$都是从同一分布中独立抽取的。此外，让我们假设该分布具有零均值和方差$\sigma^2$。请注意，这并不意味着分布必须是高斯的，只是均值和方差需要存在。现在，让我们假设层$x_j$的输入也具有零均值和方差$\gamma^2$，并且它们独立于$w_{ij}$并且彼此独立。在这种情况下，我们可以按如下方式计算$o_i$的平均值和方差：</p>

\[\begin{aligned}

    E[o_i] &amp; = \sum_{j=1}^{n_\mathrm{in}} E[w_{ij} x_j] \\&amp;= \sum_{j=1}^{n_\mathrm{in}} E[w_{ij}] E[x_j] \\&amp;= 0, \\
    \mathrm{Var}[o_i] &amp; = E[o_i^2] - (E[o_i])^2 \\
        &amp; = \sum_{j=1}^{n_\mathrm{in}} E[w^2_{ij} x^2_j] - 0 \\
        &amp; = \sum_{j=1}^{n_\mathrm{in}} E[w^2_{ij}] E[x^2_j] \\
        &amp; = n_\mathrm{in} \sigma^2 \gamma^2.

\end{aligned}\]

<p>保持方差不变的一种方法是设置$n_\mathrm{in} \sigma^2 = 1$。现在考虑反向传播过程，我们面临着类似的问题，尽管梯度是从更靠近输出的层传播的。使用与正向传播相同的推理，我们可以看到，除非$n_\mathrm{out} \sigma^2 = 1$，否则梯度的方差可能会增大，其中$n_\mathrm{out}$是该层的输出的数量。这使我们进退两难：我们不可能同时满足这两个条件。相反，我们只需满足：</p>

\[\begin{aligned}
\frac{1}{2} (n_\mathrm{in} + n_\mathrm{out}) \sigma^2 = 1 \text{ 或等价于 }
\sigma = \sqrt{\frac{2}{n_\mathrm{in} + n_\mathrm{out}}}.
\end{aligned}\]

<p>这就是现在标准且实用的<em>Xavier初始化</em>的基础，它以其提出者 <code class="language-plaintext highlighter-rouge">Glorot. Bengio.2010</code> 第一作者的名字命名。通常，Xavier初始化从均值为零，方差$\sigma^2 = \frac{2}{n_\mathrm{in} + n_\mathrm{out}}$的高斯分布中采样权重。我们也可以利用Xavier的直觉来选择从均匀分布中抽取权重时的方差。注意均匀分布$U(-a, a)$的方差为$\frac{a^2}{3}$。将$\frac{a^2}{3}$代入到$\sigma^2$的条件中，将得到初始化的建议：</p>

\[U\left(-\sqrt{\frac{6}{n_\mathrm{in} + n_\mathrm{out}}}, \sqrt{\frac{6}{n_\mathrm{in} + n_\mathrm{out}}}\right).\]

<p>尽管上述数学推理中，不存在非线性的假设在神经网络中很容易被违反，但Xavier初始化方法在实践中被证明是有效的。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">xavier</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">:</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="n">weight</span><span class="p">)</span>
</code></pre></div></div>

<div align="center"><img src="/images/常用激活函数.png" width="  " /></div>

<h4 id="k-flod">K-flod</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_k_fold_data</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">k</span> <span class="o">&gt;</span> <span class="mi">1</span>
    <span class="n">fold_size</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">k</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="n">j</span> <span class="o">*</span> <span class="n">fold_size</span><span class="p">,</span> <span class="p">(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">fold_size</span><span class="p">)</span>
        <span class="n">X_part</span><span class="p">,</span> <span class="n">y_part</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="p">:],</span> <span class="n">y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">j</span> <span class="o">==</span> <span class="n">i</span><span class="p">:</span>
            <span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="n">X_part</span><span class="p">,</span> <span class="n">y_part</span>
        <span class="k">elif</span> <span class="n">X_train</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">X_part</span><span class="p">,</span> <span class="n">y_part</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">X_train</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_part</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_part</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span>

<span class="k">def</span> <span class="nf">k_fold</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">weight_decay</span><span class="p">,</span>
           <span class="n">batch_size</span><span class="p">):</span>
    <span class="n">train_l_sum</span><span class="p">,</span> <span class="n">valid_l_sum</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">get_k_fold_data</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="n">net</span> <span class="o">=</span> <span class="n">get_net</span><span class="p">()</span>
        <span class="n">train_ls</span><span class="p">,</span> <span class="n">valid_ls</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="o">*</span><span class="n">data</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span>
                                   <span class="n">weight_decay</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">train_l_sum</span> <span class="o">+=</span> <span class="n">train_ls</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">valid_l_sum</span> <span class="o">+=</span> <span class="n">valid_ls</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">d2l</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)),</span> <span class="p">[</span><span class="n">train_ls</span><span class="p">,</span> <span class="n">valid_ls</span><span class="p">],</span>
                     <span class="n">xlabel</span><span class="o">=</span><span class="s">'epoch'</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s">'rmse'</span><span class="p">,</span> <span class="n">xlim</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">],</span>
                     <span class="n">legend</span><span class="o">=</span><span class="p">[</span><span class="s">'train'</span><span class="p">,</span> <span class="s">'valid'</span><span class="p">],</span> <span class="n">yscale</span><span class="o">=</span><span class="s">'log'</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'fold </span><span class="si">{</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">, train log rmse </span><span class="si">{</span><span class="nb">float</span><span class="p">(</span><span class="n">train_ls</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span><span class="n">f</span><span class="si">}</span><span class="s">, '</span>
              <span class="sa">f</span><span class="s">'valid log rmse </span><span class="si">{</span><span class="nb">float</span><span class="p">(</span><span class="n">valid_ls</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">train_l_sum</span> <span class="o">/</span> <span class="n">k</span><span class="p">,</span> <span class="n">valid_l_sum</span> <span class="o">/</span> <span class="n">k</span>
</code></pre></div></div>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="https://zh-v2.d2l.ai/">d2l website</a></li>
  <li><a href="https://space.bilibili.com/1567748478/channel/seriesdetail?sid=358497&amp;ctype=0">bilibili course recording</a></li>
</ul>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:MLE" role="doc-endnote">
      <p><a href="https://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1">MLE</a> <a href="#fnref:MLE" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
</div><section class="article__sharing d-print-none"></section><div class="d-print-none"><footer class="article__footer"><span>Last updated
      <time itemprop="dateModified" datetime="2021-11-22T00:00:00+08:00">Nov 22, 2021</time>
    </span><!-- start custom article footer snippet -->

<!-- end custom article footer snippet -->
<div class="article__license"><div class="license">
    <p>This work is licensed under a <a itemprop="license" rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">Attribution-NonCommercial 4.0 International</a> license.
      <a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">
        <img alt="Attribution-NonCommercial 4.0 International" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" />
      </a>
    </p>
  </div></div></footer>
<div class="article__section-navigator clearfix"><div class="previous"><span>PREVIOUS</span><a href="/2021/10/14/frp-reverse.html">frp proxy</a></div><div class="next"><span>NEXT</span><a href="/2021/11/11/Papers.html">3D Point Cloud Papers</a></div></div></div>

</div>

<script>(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    $(function() {
      var $this ,$scroll;
      var $articleContent = $('.js-article-content');
      var hasSidebar = $('.js-page-root').hasClass('layout--page--sidebar');
      var scroll = hasSidebar ? '.js-page-main' : 'html, body';
      $scroll = $(scroll);

      $articleContent.find('.highlight').each(function() {
        $this = $(this);
        $this.attr('data-lang', $this.find('code').attr('data-lang'));
      });
      $articleContent.find('h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]').each(function() {
        $this = $(this);
        $this.append($('<a class="anchor d-print-none" aria-hidden="true"></a>').html('<i class="fas fa-anchor"></i>'));
      });
      $articleContent.on('click', '.anchor', function() {
        $scroll.scrollToAnchor('#' + $(this).parent().attr('id'), 400);
      });
    });
  });
})();
</script>
</div><section class="page__comments d-print-none"><!-- fix text color in the input textarea of gitalk -->
	<style type="text/css">
		.gitalk-wrapper .gt-header-textarea {
			color: #333 !important;
		}
	</style><div class="gitalk-wrapper" id="js-gitalk-container"></div><script>
		window.Lazyload.css('https://unpkg.com/gitalk@latest/dist/gitalk.css');
		window.Lazyload.js('https://unpkg.com/gitalk@latest/dist/gitalk.min.js', function() {
			var gitalk = new Gitalk({
				clientID: '1b54ddaa46ce890821fc',
				clientSecret: '9a9d2756b1df6c10d110551e3923f2f98eb8155e',
				repo: 'gitalk',
				owner: 'Babylonehy',
				// proxy: "",
				admin: ['Babylonehy'],
				id: '/2021/11/11/D2L'
			});
			gitalk.render('js-gitalk-container');
		});
	</script></section></article><!-- start custom main bottom snippet -->

<!-- end custom main bottom snippet -->
</div>
            </div></div></div><div class="page__footer d-print-none">
<footer class="footer py-4 js-page-footer">
  <div class="main"><div itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Xiang Li (Sean)"><meta itemprop="url" content="/"><meta itemprop="description" content="I am an amazing person."><div class="footer__author-links"><div class="author-links">
  <ul class="menu menu--nowrap menu--inline"><li title="Send me an Email.">
      <a class="button button--circle mail-button" itemprop="email" href="mailto:lixiangwhuerhy@gmail,com" target="_blank">
        <i class="fas fa-envelope"></i>
      </a><li title="Follow me on Github.">
        <a class="button button--circle github-button" itemprop="sameAs" href="https://github.com/babylonehy" target="_blank">
          <div class="icon"><svg fill="#000000" width="24px" height="24px" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg">
  <path class="svgpath" data-index="path_0" fill="#272636" d="M0 525.2c0 223.6 143.3 413.7 343 483.5 26.9 6.8 22.8-12.4 22.8-25.4l0-88.7c-155.3 18.2-161.5-84.6-172-101.7-21.1-36-70.8-45.2-56-62.3 35.4-18.2 71.4 4.6 113.1 66.3 30.2 44.7 89.1 37.2 119 29.7 6.5-26.9 20.5-50.9 39.7-69.6C248.8 728.2 181.7 630 181.7 513.2c0-56.6 18.7-108.7 55.3-150.7-23.3-69.3 2.2-128.5 5.6-137.3 66.5-6 135.5 47.6 140.9 51.8 37.8-10.2 80.9-15.6 129.1-15.6 48.5 0 91.8 5.6 129.8 15.9 12.9-9.8 77-55.8 138.8-50.2 3.3 8.8 28.2 66.7 6.3 135 37.1 42.1 56 94.6 56 151.4 0 117-67.5 215.3-228.8 243.7 26.9 26.6 43.6 63.4 43.6 104.2l0 128.8c0.9 10.3 0 20.5 17.2 20.5C878.1 942.4 1024 750.9 1024 525.3c0-282.9-229.3-512-512-512C229.1 13.2 0 242.3 0 525.2L0 525.2z" />
</svg>
</div>
        </a>
      </li></ul>
</div>
</div>
    </div><div class="site-info mt-2">
      <div>© Xiang Li (Sean) 2019 - 2021,
        Powered by <a title="Jekyll is a simple, blog-aware, static site generator." href="http://jekyllrb.com/">Jekyll</a> & <a
        title="TeXt is a super customizable Jekyll theme." href="https://github.com/kitian616/jekyll-TeXt-theme">TeXt Theme</a>. </br>
        <a href="https://beian.miit.gov.cn/" target="_blank">粤ICP备2021139948号</a> </br>
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
        <span id="busuanzi_container_site_uv">本站访客数<span id="busuanzi_value_site_uv"></span>人次</span>
      </div>
    </div>
  </div>
</footer>
</div></div>
    </div><script>(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    var $body = $('body'), $window = $(window);
    var $pageRoot = $('.js-page-root'), $pageMain = $('.js-page-main');
    var activeCount = 0;
    function modal(options) {
      var $root = this, visible, onChange, hideWhenWindowScroll = false;
      var scrollTop;
      function setOptions(options) {
        var _options = options || {};
        visible = _options.initialVisible === undefined ? false : show;
        onChange = _options.onChange;
        hideWhenWindowScroll = _options.hideWhenWindowScroll;
      }
      function init() {
        setState(visible);
      }
      function setState(isShow) {
        if (isShow === visible) {
          return;
        }
        visible = isShow;
        if (visible) {
          activeCount++;
          scrollTop = $(window).scrollTop() || $pageMain.scrollTop();
          $root.addClass('modal--show');
          $pageMain.scrollTop(scrollTop);
          activeCount === 1 && ($pageRoot.addClass('show-modal'), $body.addClass('of-hidden'));
          hideWhenWindowScroll && window.hasEvent('touchstart') && $window.on('scroll', hide);
          $window.on('keyup', handleKeyup);
        } else {
          activeCount > 0 && activeCount--;
          $root.removeClass('modal--show');
          $window.scrollTop(scrollTop);
          activeCount === 0 && ($pageRoot.removeClass('show-modal'), $body.removeClass('of-hidden'));
          hideWhenWindowScroll && window.hasEvent('touchstart') && $window.off('scroll', hide);
          $window.off('keyup', handleKeyup);
        }
        onChange && onChange(visible);
      }
      function show() {
        setState(true);
      }
      function hide() {
        setState(false);
      }
      function handleKeyup(e) {
        // Char Code: 27  ESC
        if (e.which ===  27) {
          hide();
        }
      }
      setOptions(options);
      init();
      return {
        show: show,
        hide: hide,
        $el: $root
      };
    }
    $.fn.modal = modal;
  });
})();
</script><div class="modal modal--overflow page__search-modal d-print-none js-page-search-modal"><script>window.useDefaultSearchBox = false;</script><script>
(function () {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    // search panel
    var search = (window.search || (window.search = {}));
    var useDefaultSearchBox = window.useDefaultSearchBox === undefined ?
      true : window.useDefaultSearchBox ;

    var $searchModal = $('.js-page-search-modal');
    var $searchToggle = $('.js-search-toggle');
    var searchModal = $searchModal.modal({ onChange: handleModalChange, hideWhenWindowScroll: true });
    var modalVisible = false;
    search.searchModal = searchModal;

    var $searchBox = null;
    var $searchInput = null;
    var $searchClear = null;

    function getModalVisible() {
      return modalVisible;
    }
    search.getModalVisible = getModalVisible;

    function handleModalChange(visible) {
      modalVisible = visible;
      if (visible) {
        search.onShow && search.onShow();
        useDefaultSearchBox && $searchInput[0] && $searchInput[0].focus();
      } else {
        search.onShow && search.onHide();
        useDefaultSearchBox && $searchInput[0] && $searchInput[0].blur();
        setTimeout(function() {
          useDefaultSearchBox && ($searchInput.val(''), $searchBox.removeClass('not-empty'));
          search.clear && search.clear();
          window.pageAsideAffix && window.pageAsideAffix.refresh();
        }, 400);
      }
    }

    $searchToggle.on('click', function() {
      modalVisible ? searchModal.hide() : searchModal.show();
    });
    // Char Code: 83  S, 191 /
    $(window).on('keyup', function(e) {
      if (!modalVisible && !window.isFormElement(e.target || e.srcElement) && (e.which === 83 || e.which === 191)) {
        modalVisible || searchModal.show();
      }
    });

    if (useDefaultSearchBox) {
      $searchBox = $('.js-search-box');
      $searchInput = $searchBox.children('input');
      $searchClear = $searchBox.children('.js-icon-clear');
      search.getSearchInput = function() {
        return $searchInput.get(0);
      };
      search.getVal = function() {
        return $searchInput.val();
      };
      search.setVal = function(val) {
        $searchInput.val(val);
      };

      $searchInput.on('focus', function() {
        $(this).addClass('focus');
      });
      $searchInput.on('blur', function() {
        $(this).removeClass('focus');
      });
      $searchInput.on('input', window.throttle(function() {
        var val = $(this).val();
        if (val === '' || typeof val !== 'string') {
          search.clear && search.clear();
        } else {
          $searchBox.addClass('not-empty');
          search.onInputNotEmpty && search.onInputNotEmpty(val);
        }
      }, 400));
      $searchClear.on('click', function() {
        $searchInput.val(''); $searchBox.removeClass('not-empty');
        search.clear && search.clear();
      });
    }
  });
})();
</script></div></div>


<script>(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    function scrollToAnchor(anchor, duration, callback) {
      var $root = this;
      $root.animate({ scrollTop: $(anchor).position().top }, duration, function() {
        window.history.replaceState(null, '', window.location.href.split('#')[0] + anchor);
        callback && callback();
      });
    }
    $.fn.scrollToAnchor = scrollToAnchor;
  });
})();
(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    function affix(options) {
      var $root = this, $window = $(window), $scrollTarget, $scroll,
        offsetBottom = 0, scrollTarget = window, scroll = window.document, disabled = false, isOverallScroller = true,
        rootTop, rootLeft, rootHeight, scrollBottom, rootBottomTop,
        hasInit = false, curState;

      function setOptions(options) {
        var _options = options || {};
        _options.offsetBottom && (offsetBottom = _options.offsetBottom);
        _options.scrollTarget && (scrollTarget = _options.scrollTarget);
        _options.scroll && (scroll = _options.scroll);
        _options.disabled !== undefined && (disabled = _options.disabled);
        $scrollTarget = $(scrollTarget);
        isOverallScroller = window.isOverallScroller($scrollTarget[0]);
        $scroll = $(scroll);
      }
      function preCalc() {
        top();
        rootHeight = $root.outerHeight();
        rootTop = $root.offset().top + (isOverallScroller ? 0 :  $scrollTarget.scrollTop());
        rootLeft = $root.offset().left;
      }
      function calc(needPreCalc) {
        needPreCalc && preCalc();
        scrollBottom = $scroll.outerHeight() - offsetBottom - rootHeight;
        rootBottomTop = scrollBottom - rootTop;
      }
      function top() {
        if (curState !== 'top') {
          $root.removeClass('fixed').css({
            left: 0,
            top: 0
          });
          curState = 'top';
        }
      }
      function fixed() {
        if (curState !== 'fixed') {
          $root.addClass('fixed').css({
            left: rootLeft + 'px',
            top: 0
          });
          curState = 'fixed';
        }
      }
      function bottom() {
        if (curState !== 'bottom') {
          $root.removeClass('fixed').css({
            left: 0,
            top: rootBottomTop + 'px'
          });
          curState = 'bottom';
        }
      }
      function setState() {
        var scrollTop = $scrollTarget.scrollTop();
        if (scrollTop >= rootTop && scrollTop <= scrollBottom) {
          fixed();
        } else if (scrollTop < rootTop) {
          top();
        } else {
          bottom();
        }
      }
      function init() {
        if(!hasInit) {
          var interval, timeout;
          calc(true); setState();
          // run calc every 100 millisecond
          interval = setInterval(function() {
            calc();
          }, 100);
          timeout = setTimeout(function() {
            clearInterval(interval);
          }, 45000);
          window.pageLoad.then(function() {
            setTimeout(function() {
              clearInterval(interval);
              clearTimeout(timeout);
            }, 3000);
          });
          $scrollTarget.on('scroll', function() {
            disabled || setState();
          });
          $window.on('resize', function() {
            disabled || (calc(true), setState());
          });
          hasInit = true;
        }
      }

      setOptions(options);
      if (!disabled) {
        init();
      }
      $window.on('resize', window.throttle(function() {
        init();
      }, 200));
      return {
        setOptions: setOptions,
        refresh: function() {
          calc(true, { animation: false }); setState();
        }
      };
    }
    $.fn.affix = affix;
  });
})();
(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    function toc(options) {
      var $root = this, $window = $(window), $scrollTarget, $scroller, $tocUl = $('<ul class="toc toc--ellipsis"></ul>'), $tocLi, $headings, $activeLast, $activeCur,
        selectors = 'h1,h2,h3', container = 'body', scrollTarget = window, scroller = 'html, body', disabled = false,
        headingsPos, scrolling = false, hasRendered = false, hasInit = false;

      function setOptions(options) {
        var _options = options || {};
        _options.selectors && (selectors = _options.selectors);
        _options.container && (container = _options.container);
        _options.scrollTarget && (scrollTarget = _options.scrollTarget);
        _options.scroller && (scroller = _options.scroller);
        _options.disabled !== undefined && (disabled = _options.disabled);
        $headings = $(container).find(selectors).filter('[id]');
        $scrollTarget = $(scrollTarget);
        $scroller = $(scroller);
      }
      function calc() {
        headingsPos = [];
        $headings.each(function() {
          headingsPos.push(Math.floor($(this).position().top));
        });
      }
      function setState(element, disabled) {
        var scrollTop = $scrollTarget.scrollTop(), i;
        if (disabled || !headingsPos || headingsPos.length < 1) { return; }
        if (element) {
          $activeCur = element;
        } else {
          for (i = 0; i < headingsPos.length; i++) {
            if (scrollTop >= headingsPos[i]) {
              $activeCur = $tocLi.eq(i);
            } else {
              $activeCur || ($activeCur = $tocLi.eq(i));
              break;
            }
          }
        }
        $activeLast && $activeLast.removeClass('active');
        ($activeLast = $activeCur).addClass('active');
      }
      function render() {
        if(!hasRendered) {
          $root.append($tocUl);
          $headings.each(function() {
            var $this = $(this);
            $tocUl.append($('<li></li>').addClass('toc-' + $this.prop('tagName').toLowerCase())
              .append($('<a></a>').text($this.text()).attr('href', '#' + $this.prop('id'))));
          });
          $tocLi = $tocUl.children('li');
          $tocUl.on('click', 'a', function(e) {
            e.preventDefault();
            var $this = $(this);
            scrolling = true;
            setState($this.parent());
            $scroller.scrollToAnchor($this.attr('href'), 400, function() {
              scrolling = false;
            });
          });
        }
        hasRendered = true;
      }
      function init() {
        var interval, timeout;
        if(!hasInit) {
          render(); calc(); setState(null, scrolling);
          // run calc every 100 millisecond
          interval = setInterval(function() {
            calc();
          }, 100);
          timeout = setTimeout(function() {
            clearInterval(interval);
          }, 45000);
          window.pageLoad.then(function() {
            setTimeout(function() {
              clearInterval(interval);
              clearTimeout(timeout);
            }, 3000);
          });
          $scrollTarget.on('scroll', function() {
            disabled || setState(null, scrolling);
          });
          $window.on('resize', window.throttle(function() {
            if (!disabled) {
              render(); calc(); setState(null, scrolling);
            }
          }, 100));
        }
        hasInit = true;
      }

      setOptions(options);
      if (!disabled) {
        init();
      }
      $window.on('resize', window.throttle(function() {
        init();
      }, 200));
      return {
        setOptions: setOptions
      };
    }
    $.fn.toc = toc;
  });
})();
/*(function () {

})();*/
</script><script>
  /* toc must before affix, since affix need to konw toc' height. */(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  var TOC_SELECTOR = window.TEXT_VARIABLES.site.toc.selectors;
  window.Lazyload.js(SOURCES.jquery, function() {
    var $window = $(window);
    var $articleContent = $('.js-article-content');
    var $tocRoot = $('.js-toc-root'), $col2 = $('.js-col-aside');
    var toc;
    var tocDisabled = false;
    var hasSidebar = $('.js-page-root').hasClass('layout--page--sidebar');
    var hasToc = $articleContent.find(TOC_SELECTOR).length > 0;

    function disabled() {
      return $col2.css('display') === 'none' || !hasToc;
    }

    tocDisabled = disabled();

    toc = $tocRoot.toc({
      selectors: TOC_SELECTOR,
      container: $articleContent,
      scrollTarget: hasSidebar ? '.js-page-main' : null,
      scroller: hasSidebar ? '.js-page-main' : null,
      disabled: tocDisabled
    });

    $window.on('resize', window.throttle(function() {
      tocDisabled = disabled();
      toc && toc.setOptions({
        disabled: tocDisabled
      });
    }, 100));

  });
})();
(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    var $window = $(window), $pageFooter = $('.js-page-footer');
    var $pageAside = $('.js-page-aside');
    var affix;
    var tocDisabled = false;
    var hasSidebar = $('.js-page-root').hasClass('layout--page--sidebar');

    affix = $pageAside.affix({
      offsetBottom: $pageFooter.outerHeight(),
      scrollTarget: hasSidebar ? '.js-page-main' : null,
      scroller: hasSidebar ? '.js-page-main' : null,
      scroll: hasSidebar ? $('.js-page-main').children() : null,
      disabled: tocDisabled
    });

    $window.on('resize', window.throttle(function() {
      affix && affix.setOptions({
        disabled: tocDisabled
      });
    }, 100));

    window.pageAsideAffix = affix;
  });
})();
</script><script type="text/x-mathjax-config">
	var _config = { tex2jax: {
		inlineMath: [['$','$'], ['\\(','\\)']],
		skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code','details'] 
	}};_config.TeX = { equationNumbers: { autoNumber: "all" } };MathJax.Hub.Config(_config);
</script>

<script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
<script>
  window.Lazyload.js('https://cdn.bootcss.com/mermaid/8.0.0-rc.8/mermaid.min.js', function() {
    mermaid.initialize({
      startOnLoad: true
    });
    mermaid.init(undefined, '.language-mermaid');
  });
</script>
<script>(function() {
  function errorHandler(error, callback) {
    if (error) {
      callback && callback(error);
      throw error;
    }
  }

  function pageview(_AV, options) {
    var AV = _AV;
    var appId, appKey, appClass;
    appId = options.appId;
    appKey = options.appKey;
    appClass = options.appClass;
    AV.init({
      // serverURLs: 'https://avoscloud.com',
      appId: appId,
      appKey: appKey
    });
    return {
      get: get,
      increase: increase
    };

    function searchKey(key) {
      var query = new AV.Query(appClass);
      query.equalTo('key', key);
      return query.first();
    }

    function insert(key, title) {
      var Blog = AV.Object.extend(appClass);
      var blog = new Blog();
      blog.set('title', title);
      blog.set('key', key);
      blog.set('views', 0);
      return blog.save();
    }

    function increment(result) {
      result.increment('views', 1);
      return result.save(null, {
        fetchWhenSave: true
      });
    }

    function get(key, callback) {
      searchKey(key).then(function(result) {
        if (result) {
          callback && callback(result.attributes.views);
        }
      }, errorHandler);
    }

    function increase(key, title, callback) {
      searchKey(key).then(function(result) {
        if (result) {
          increment(result).then(function(result) {
            callback && callback(result.attributes.views);
          });
        } else {
          insert(key, title).then(function(result) {
            increment(result).then(function(result) {
              callback && callback(result.attributes.views);
            });
          }, errorHandler);
        }
      }, errorHandler);
    }
  }
  window.pageview = pageview;
})();
</script>
  <script>
    window.Lazyload.js(['https://cdn.bootcss.com/jquery/3.1.1/jquery.min.js', '//cdn.jsdelivr.net/npm/leancloud-storage@3.13.2/dist/av-min.js'], function() {
      var pageview = window.pageview(AV, {
        appId:    's5aOdbT9tUPOJ6iPLwiejPCe-MdYXbMMI',
        appKey:   'P2lS3d7WyB0HydCooFWVzkh5',
        appClass: 'comments'
      });
      var key =   '/2021/11/11/D2L';
      var title = "Dive into Deep Learning Notes(1)";
      pageview.increase(key, title, function(view) {
        $("[data-page-key='/2021/11/11/D2L']").text(view);
      });
    });
  </script>
    </div>
    <script>(function () {
  var $root = document.getElementsByClassName('root')[0];
  if (window.hasEvent('touchstart')) {
    $root.dataset.isTouch = true;
    document.addEventListener('touchstart', function(){}, false);
  }
})();
</script>
  </body>
</html>

